{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d09a5f0",
   "metadata": {},
   "source": [
    "---\n",
    "## **📊 Flow**   \n",
    "> step01 : Data   \n",
    "> - 데이터 준비 및 분석   \n",
    "> - 데이터 전처리\n",
    "   #   \n",
    "> setp02 : 모델 비교     \n",
    "> - ResNet50, EfficientNet,VGG16\n",
    "> - 모델 학습\n",
    "> - 성능평가 : 정확도 재현율 f1\n",
    "   #\n",
    "> setp03 : 분석 실험   \n",
    "> - Focal Loss, Class-balanced Loss 적용.   \n",
    "> - Precision-Recall Trade-off   \n",
    "> - XAI 기법   \n",
    "> - 오분류 분석   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bdfc80",
   "metadata": {},
   "source": [
    "---\n",
    "> ### step01 : Data   \n",
    "> - 데이터 준비 및 분석   \n",
    "> - 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed597957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprocessing import *\n",
    "\n",
    "# 데이터 경로\n",
    "data_path = '/home/dibaeck/sketch/study_Data4Quality/task02_CovidClassifier/COVID19_1K'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd80077",
   "metadata": {},
   "source": [
    "---\n",
    ">> 데이터 사이즈 다름 --> TASK : 데이터 리사이즈  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84997161",
   "metadata": {},
   "outputs": [],
   "source": [
    "check_image_sizes(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f345db94",
   "metadata": {},
   "source": [
    "---\n",
    ">> 클래스 불균형 : COVID19 데이터가 적음.  --> TASK : Data Augmentation   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eb00a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_and_visualize_images(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9793bbb",
   "metadata": {},
   "source": [
    "클래스 불균형 해결 전략 : WeightedRandomSampler + FocalLoss\n",
    "\n",
    "| 전략               | 설명                                                                 |\n",
    "|--------------------|----------------------------------------------------------------------|\n",
    "| **WeightedRandomSampler**          | 수 클래스를 더 자주 뽑히게 하는 샘플링 방식                |\n",
    "| **Focal Loss**     | 소수 클래스의 어려운 샘플에 더 집중하는 loss function(의료 이미지에서 성능 개선에 효과적)                               |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503ffc26",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abac0d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################## setting \n",
    "from dataprocessing import *\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "# 데이터 경로\n",
    "data_path = '/home/dibaeck/sketch/study_Data4Quality/task02_CovidClassifier/COVID19_1K'\n",
    "\n",
    "# 데이터 정규화 및 리사이즈, 텐서화\n",
    "transform = get_transform()\n",
    "\n",
    "no1_target_classes = ['PNEUMONIA', 'NORMAL']\n",
    "train_dataset1, val_dataset1, test_dataset1 = get_customdatasets(data_path, transform,no1_target_classes)\n",
    "train_loader1,val_loader1,test_loader1 = get_dataloaders(train_dataset1,val_dataset1,test_dataset1,batch_size=32)\n",
    "\n",
    "no2_target_classes = ['COVID19', 'NORMAL']\n",
    "train_dataset2, val_dataset2, test_dataset2 = get_customdatasets(data_path, transform,no2_target_classes)\n",
    "train_loader2,val_loader2,test_loader2 = get_dataloaders(train_dataset2,val_dataset2,test_dataset2,batch_size=32)\n",
    "\n",
    "no3_target_classes = ['NORMAL',\"PNEUMONIA\",'COVID19']\n",
    "train_dataset3, val_dataset3, test_dataset3 = get_customdatasets(data_path, transform,no3_target_classes)\n",
    "train_loader3,val_loader3,test_loader3 = get_dataloaders(train_dataset2,val_dataset2,test_dataset2,batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e779971a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dibaeck/sketch/anaconda3/envs/dibk311/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/dibaeck/sketch/anaconda3/envs/dibk311/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/densenet121-a639ec97.pth\" to /home/dibaeck/.cache/torch/hub/checkpoints/densenet121-a639ec97.pth\n",
      "100%|██████████| 30.8M/30.8M [00:00<00:00, 56.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "from models import *\n",
    "import torch.optim as optim\n",
    "\n",
    "# 모델 불러오기\n",
    "# Binary Classification (PNEUMONIA vs NORMAL)\n",
    "binary_model_pneumonia_vs_normal = CustomDenseNet(num_classes=2)\n",
    "\n",
    "# Binary Classification (COVID19 vs NORMAL)\n",
    "binary_model_covid19_vs_normal = CustomDenseNet(num_classes=2)\n",
    "\n",
    "# Multi-class Classification (COVID19 vs PNEUMONIA vs NORMAL)\n",
    "multi_class_model = CustomDenseNet(num_classes=3)\n",
    "\n",
    "\n",
    "# 지표 설정\n",
    "# 각각의 모델에 대해 이진 분류에서는 Binary Cross Entropy를, 다중 클래스 분류에서는 Cross Entropy를 사용\n",
    "# 이진 분류 모델 (PNEUMONIA vs NORMAL, COVID19 vs NORMAL) \n",
    "criterion_binary = nn.BCEWithLogitsLoss()  # 이진 분류에서 사용\n",
    "optimizer_binary = optim.Adam(binary_model_pneumonia_vs_normal.parameters(), lr=0.001)\n",
    "\n",
    "# 다중 클래스 분류 모델 (COVID19 vs PNEUMONIA vs NORMAL)\n",
    "criterion_multi_class = nn.CrossEntropyLoss()  # 다중 클래스 분류에서 사용\n",
    "optimizer_multi_class = optim.Adam(multi_class_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d233fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 224, 224])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "GET was unable to find an engine to execute this computation",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 모델 학습\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mbinary_model_pneumonia_vs_normal\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmodel_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion_binary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_binary\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sketch/study_Data4Quality/task02_CovidClassifier/models.py:53\u001b[39m, in \u001b[36mCustomDenseNet.model_train\u001b[39m\u001b[34m(self, train_loader, criterion, optimizer, num_epochs)\u001b[39m\n\u001b[32m     51\u001b[39m labels = labels.to(\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m     52\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_classes == \u001b[32m2\u001b[39m:       \u001b[38;5;66;03m# 이진분류\u001b[39;00m\n\u001b[32m     56\u001b[39m     labels = labels.float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sketch/anaconda3/envs/dibk311/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1499\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1500\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[32m   1503\u001b[39m full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sketch/study_Data4Quality/task02_CovidClassifier/models.py:35\u001b[39m, in \u001b[36mCustomDenseNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdensenet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sketch/anaconda3/envs/dibk311/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1499\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1500\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[32m   1503\u001b[39m full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sketch/anaconda3/envs/dibk311/lib/python3.11/site-packages/torchvision/models/densenet.py:213\u001b[39m, in \u001b[36mDenseNet.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    212\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m     out = F.relu(features, inplace=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    215\u001b[39m     out = F.adaptive_avg_pool2d(out, (\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sketch/anaconda3/envs/dibk311/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1499\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1500\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[32m   1503\u001b[39m full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sketch/anaconda3/envs/dibk311/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sketch/anaconda3/envs/dibk311/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1496\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1497\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1498\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1499\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1500\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1502\u001b[39m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[32m   1503\u001b[39m full_backward_hooks, non_full_backward_hooks = [], []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sketch/anaconda3/envs/dibk311/lib/python3.11/site-packages/torch/nn/modules/conv.py:463\u001b[39m, in \u001b[36mConv2d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    462\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m463\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/sketch/anaconda3/envs/dibk311/lib/python3.11/site-packages/torch/nn/modules/conv.py:459\u001b[39m, in \u001b[36mConv2d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m'\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    456\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv2d(F.pad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode),\n\u001b[32m    457\u001b[39m                     weight, bias, \u001b[38;5;28mself\u001b[39m.stride,\n\u001b[32m    458\u001b[39m                     _pair(\u001b[32m0\u001b[39m), \u001b[38;5;28mself\u001b[39m.dilation, \u001b[38;5;28mself\u001b[39m.groups)\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: GET was unable to find an engine to execute this computation"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "binary_model_pneumonia_vs_normal.model_train(train_loader1, criterion_binary, optimizer_binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115ec338",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_model_covid19_vs_normal.model_train(train_loader2, criterion_binary, optimizer_binary)\n",
    "\n",
    "multi_class_model.model_train(train_loader3,criterion_multi_class,optimizer_multi_class)\n",
    "뭐가 문제인데ㅜㅠㅜㅠㅜㅠㅜ\n",
    "\n",
    "\n",
    "# 모델 평가\n",
    "binary_model_pneumonia_vs_normal.model_eval(test_loader)\n",
    "binary_model_covid19_vs_normal.model_eval(test_loader)\n",
    "multi_class_model.model_eval(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed3ca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련 및 평가 실행\n",
    "train_and_evaluate_models(train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a879e7a3",
   "metadata": {},
   "source": [
    "### 모델 성능 비교\n",
    "\n",
    "| **모델**                          | **정확도 (Accuracy)** | **정밀도 (Precision)** | **재현율 (Recall)** | **F1-Score** | **Confusion Matrix**                                |\n",
    "|-----------------------------------|----------------------|------------------------|---------------------|--------------|-----------------------------------------------------|\n",
    "| **PNEUMONIA vs NORMAL**           | 0.9655               | 0.9286                 | 0.6842              | 0.7879       | [[183   1] <br> [  6  13]]                         |\n",
    "| **COVID19 vs NORMAL**             | 0.9360               | 0.9744                 | 0.7600              | 0.8539       | [[152   1] <br> [ 12  38]]                         |\n",
    "| **Multi-class (COVID19 vs PNEUMONIA vs NORMAL)** | 0.9064 | 0.9064 | 0.9064 | 0.9064 | [[ 12   1   6] <br> [  1  39  10] <br> [  1   0 133]] |\n",
    "\n",
    "---\n",
    "\n",
    "### 결론\n",
    "\n",
    "- **PNEUMONIA vs NORMAL 모델**: 높은 정확도와 정밀도, 하지만 재현율이 상대적으로 낮습니다.\n",
    "- **COVID19 vs NORMAL 모델**: 높은 정확도, 정밀도, 재현율을 보이며, 비교적 균형 잡힌 성능을 보여줍니다.\n",
    "- **Multi-class 모델**: 세 가지 클래스에 대해 고른 성능을 보여주며, F1-Score와 정확도가 90% 이상으로 우수합니다.\n",
    "\n",
    "**최종 평가**:\n",
    "- **COVID19 vs NORMAL 모델**은 정밀도와 재현율이 균형을 이루어 가장 우수한 성능을 보입니다.\n",
    "- **Multi-class 모델**은 세 가지 클래스를 잘 분리하며 좋은 성능을 보였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1567860a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bee5d4c",
   "metadata": {},
   "source": [
    "1]Multi-class분류에서 사용된 loss 가 모든 클래스에 적절했는지 분석하고,Focal loss,class-balancedloss등을 적용한 실험을 수행해보세요.\n",
    "\n",
    "1) Loss 함수 분석 및 Focal Loss, Class-Balanced Loss 실험   \n",
    "Multi-class 분류에서 기본적으로 사용되는 loss 함수는 Cross-Entropy Loss입니다. 이를 Focal Loss나 Class-Balanced Loss로 변경하여 성능을 비교할 수 있습니다.\n",
    "   \n",
    "Focal Loss:   \n",
    "Focal Loss는 클래스 불균형 문제를 해결하기 위한 loss 함수입니다. Cross-Entropy Loss는 클래스 간 불균형이 심할 때 성능이 떨어질 수 있는데, Focal Loss는 어려운 샘플에 가중치를 주어 성능을 개선할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002292aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=0.25, num_classes=3):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.num_classes = num_classes\n",
    "        self.ce_loss = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        ce_loss = self.ce_loss(inputs, targets)\n",
    "        pt = torch.exp(-ce_loss)  # For each class\n",
    "        focal_loss = self.alpha * (1 - pt) ** self.gamma * ce_loss\n",
    "        return focal_loss.mean()\n",
    "    \n",
    "class ClassBalancedLoss(nn.Module):\n",
    "    def __init__(self, class_weights):\n",
    "        super(ClassBalancedLoss, self).__init__()\n",
    "        self.class_weights = class_weights\n",
    "        self.ce_loss = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        return self.ce_loss(inputs, targets)\n",
    "    \n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "def adjust_threshold(predictions, targets, threshold=0.5):\n",
    "    pred_classes = (predictions[:, 1] > threshold).float()  # COVID-19 클래스의 확률을 기준으로 예측\n",
    "    precision = precision_score(targets, pred_classes, average='binary', pos_label=1)\n",
    "    recall = recall_score(targets, pred_classes, average='binary', pos_label=1)\n",
    "    return precision, recall\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torchvision import models\n",
    "\n",
    "class GradCAM:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        self.gradients = None\n",
    "        self.activations = None\n",
    "\n",
    "    def save_gradient(self, grad):\n",
    "        self.gradients = grad\n",
    "\n",
    "    def forward(self, x):\n",
    "        activations = self.model.features(x)\n",
    "        activations.register_hook(self.save_gradient)\n",
    "        return activations\n",
    "\n",
    "    def generate_gradcam(self, input_image, target_class):\n",
    "        activations = self.forward(input_image)\n",
    "        self.model.zero_grad()\n",
    "        activations[target_class].backward(retain_graph=True)\n",
    "        gradients = self.gradients\n",
    "        pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])\n",
    "        weighted_activations = activations * pooled_gradients.view(1, -1, 1, 1)\n",
    "        gradcam = torch.mean(weighted_activations, dim=1).squeeze()\n",
    "        gradcam = np.maximum(gradcam.cpu().detach().numpy(), 0)\n",
    "        gradcam = cv2.resize(gradcam, (input_image.size(2), input_image.size(3)))\n",
    "        return gradcam\n",
    "\n",
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# SHAP을 사용하여 모델 해석\n",
    "def explain_with_shap(model, dataloader):\n",
    "    explainer = shap.KernelExplainer(model.predict, data=dataloader)\n",
    "    shap_values = explainer.shap_values(dataloader)\n",
    "    shap.summary_plot(shap_values, dataloader)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def analyze_confusion_matrix(predictions, targets):\n",
    "    cm = confusion_matrix(targets, predictions)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# 예시: 오분류된 샘플에 대해 가중치를 더 주기\n",
    "def reweight_loss_function(model, predictions, targets):\n",
    "    # 잘못 분류된 샘플에 대해 가중치 증가\n",
    "    incorrect_samples = predictions != targets\n",
    "    weights = torch.where(incorrect_samples, 2.0, 1.0)\n",
    "    loss = F.cross_entropy(predictions, targets, weight=weights)\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8222df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 시 사용될 loss 함수 설정\n",
    "criterion = FocalLoss(gamma=2, alpha=0.25, num_classes=3)  # 예시로 FocalLoss 사용\n",
    "\n",
    "# 학습 과정에서 해당 loss를 사용하여 모델 훈련\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bbc3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = [0.3, 0.5, 0.7]  # 여러 threshold 값 시도\n",
    "for threshold in thresholds:\n",
    "    precision, recall = adjust_threshold(predictions, targets, threshold)\n",
    "    print(f\"Threshold: {threshold} -> Precision: {precision}, Recall: {recall}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a18ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradcam = GradCAM(model)\n",
    "cam_output = gradcam.generate_gradcam(input_image, target_class)\n",
    "# 결과를 시각화\n",
    "plt.imshow(cam_output, cmap='jet')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68912131",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dibk311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
